---
phase: 01-schema-gpp-engine
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/src/db/dryingSchema.js
  - backend/src/db/apexSchema.js
autonomous: true

must_haves:
  truths:
    - "All 10 drying tables exist in SQLite after server startup"
    - "Foreign keys cascade correctly (deleting a drying_log removes all children)"
    - "Schema migrations run safely on existing databases without data loss"
    - "drying_ref_points has job-wide unique constraint on (log_id, ref_number)"
  artifacts:
    - path: "backend/src/db/dryingSchema.js"
      provides: "All drying_* table creation and migrations"
      contains: "CREATE TABLE drying_logs"
    - path: "backend/src/db/apexSchema.js"
      provides: "Require chain to load dryingSchema at startup"
      contains: "require('./dryingSchema')"
  key_links:
    - from: "backend/src/db/dryingSchema.js"
      to: "backend/src/db/schema.js"
      via: "require('./schema') to get db instance"
      pattern: "require\\('./schema'\\)"
    - from: "backend/src/db/apexSchema.js"
      to: "backend/src/db/dryingSchema.js"
      via: "require at bottom to ensure load order"
      pattern: "require\\('./dryingSchema'\\)"
    - from: "drying_logs.job_id"
      to: "apex_jobs.id"
      via: "REFERENCES with ON DELETE CASCADE"
      pattern: "REFERENCES apex_jobs\\(id\\) ON DELETE CASCADE"
---

<objective>
Create all 10 normalized drying log tables in SQLite with proper foreign keys, cascade deletes, indexes, and migration safety.

Purpose: Establish the complete data foundation for drying logs before any CRUD functions, GPP calculations, or API routes are built. Every subsequent phase depends on these tables existing.

Output: `backend/src/db/dryingSchema.js` (new file) and updated `backend/src/db/apexSchema.js` (require chain).
</objective>

<execution_context>
@C:/Users/jaker/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/jaker/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-schema-gpp-engine/01-RESEARCH.md

@backend/src/db/schema.js
@backend/src/db/apexSchema.js
@backend/src/index.js
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create dryingSchema.js with all 10 drying tables</name>
  <files>backend/src/db/dryingSchema.js</files>
  <action>
Create `backend/src/db/dryingSchema.js` following the exact pattern from `apexSchema.js`:
- `const db = require('./schema');` at top (NOT require apexSchema)
- `module.exports = db;` at bottom

Create these 10 tables using the check-existence-then-create pattern (`db.prepare("SELECT name FROM sqlite_master WHERE type='table' AND name='...'").get()` + `if (!table) { db.exec(...) }`):

**1. drying_logs** (one per job)
- id TEXT PRIMARY KEY
- job_id TEXT NOT NULL UNIQUE REFERENCES apex_jobs(id) ON DELETE CASCADE
- status TEXT DEFAULT 'active' CHECK(status IN ('active', 'complete'))
- next_ref_number INTEGER DEFAULT 1
- completed_at TEXT
- created_at TEXT DEFAULT (datetime('now'))
- updated_at TEXT DEFAULT (datetime('now'))
- INDEX: idx_drying_logs_job_id ON drying_logs(job_id) (UNIQUE)

**2. drying_chambers** (containment zones within a log)
- id TEXT PRIMARY KEY
- log_id TEXT NOT NULL REFERENCES drying_logs(id) ON DELETE CASCADE
- name TEXT NOT NULL
- color TEXT DEFAULT ''
- position INTEGER DEFAULT 0
- created_at TEXT DEFAULT (datetime('now'))
- updated_at TEXT DEFAULT (datetime('now'))
- INDEX: idx_drying_chambers_log_id

**3. drying_rooms** (rooms within chambers)
- id TEXT PRIMARY KEY
- chamber_id TEXT NOT NULL REFERENCES drying_chambers(id) ON DELETE CASCADE
- name TEXT NOT NULL
- position INTEGER DEFAULT 0
- created_at TEXT DEFAULT (datetime('now'))
- updated_at TEXT DEFAULT (datetime('now'))
- INDEX: idx_drying_rooms_chamber_id

**4. drying_ref_points** (measurement locations within rooms)
- id TEXT PRIMARY KEY
- room_id TEXT NOT NULL REFERENCES drying_rooms(id) ON DELETE CASCADE
- log_id TEXT NOT NULL REFERENCES drying_logs(id) ON DELETE CASCADE
- ref_number INTEGER NOT NULL
- material_code TEXT NOT NULL DEFAULT ''
- label TEXT DEFAULT ''
- demolished_at TEXT
- demolished_visit_id TEXT
- created_at TEXT DEFAULT (datetime('now'))
- UNIQUE(log_id, ref_number)
- INDEX: idx_drying_ref_points_room_id
- INDEX: idx_drying_ref_points_log_id

**5. drying_baselines** (target moisture per material type per log)
- id TEXT PRIMARY KEY
- log_id TEXT NOT NULL REFERENCES drying_logs(id) ON DELETE CASCADE
- material_code TEXT NOT NULL
- baseline_value REAL NOT NULL
- created_at TEXT DEFAULT (datetime('now'))
- updated_at TEXT DEFAULT (datetime('now'))
- UNIQUE(log_id, material_code)
- INDEX: idx_drying_baselines_log_id

**6. drying_visits** (timestamped site visit records)
- id TEXT PRIMARY KEY
- log_id TEXT NOT NULL REFERENCES drying_logs(id) ON DELETE CASCADE
- visit_number INTEGER NOT NULL
- visited_at TEXT NOT NULL DEFAULT (datetime('now'))
- created_at TEXT DEFAULT (datetime('now'))
- updated_at TEXT DEFAULT (datetime('now'))
- UNIQUE(log_id, visit_number)
- INDEX: idx_drying_visits_log_id

**7. drying_atmospheric_readings** (temp/RH/GPP per visit)
- id TEXT PRIMARY KEY
- visit_id TEXT NOT NULL REFERENCES drying_visits(id) ON DELETE CASCADE
- reading_type TEXT NOT NULL CHECK(reading_type IN ('chamber_intake', 'dehu_exhaust', 'unaffected', 'outside'))
- chamber_id TEXT REFERENCES drying_chambers(id) ON DELETE SET NULL
- dehu_number INTEGER
- temp_f REAL
- rh_percent REAL
- gpp REAL
- created_at TEXT DEFAULT (datetime('now'))
- INDEX: idx_drying_atmospheric_visit_id ON drying_atmospheric_readings(visit_id)

**8. drying_moisture_readings** (MC per reference point per visit)
- id TEXT PRIMARY KEY
- visit_id TEXT NOT NULL REFERENCES drying_visits(id) ON DELETE CASCADE
- ref_point_id TEXT NOT NULL REFERENCES drying_ref_points(id) ON DELETE CASCADE
- reading_value REAL
- meets_dry_standard INTEGER DEFAULT 0
- created_at TEXT DEFAULT (datetime('now'))
- UNIQUE(visit_id, ref_point_id)
- INDEX: idx_drying_moisture_visit_id ON drying_moisture_readings(visit_id)
- INDEX: idx_drying_moisture_ref_point_id ON drying_moisture_readings(ref_point_id)

**9. drying_equipment** (equipment snapshot per room per visit)
- id TEXT PRIMARY KEY
- visit_id TEXT NOT NULL REFERENCES drying_visits(id) ON DELETE CASCADE
- room_id TEXT NOT NULL REFERENCES drying_rooms(id) ON DELETE CASCADE
- equipment_type TEXT NOT NULL DEFAULT 'AM'
- quantity INTEGER DEFAULT 1
- created_at TEXT DEFAULT (datetime('now'))
- INDEX: idx_drying_equipment_visit_id ON drying_equipment(visit_id)
- INDEX: idx_drying_equipment_room_id ON drying_equipment(room_id)

**10. drying_visit_notes** (text notes with optional photos)
- id TEXT PRIMARY KEY
- visit_id TEXT NOT NULL REFERENCES drying_visits(id) ON DELETE CASCADE
- content TEXT DEFAULT ''
- photos TEXT DEFAULT '[]'
- created_at TEXT DEFAULT (datetime('now'))
- INDEX: idx_drying_visit_notes_visit_id ON drying_visit_notes(visit_id)

Use `CREATE INDEX IF NOT EXISTS` for all indexes. Add console.log messages matching existing pattern (e.g., `console.log('Creating drying_logs table...');` and `console.log('Drying logs table created');`).

End the file with `console.log('Drying logs schema initialized');` and `module.exports = db;`.
  </action>
  <verify>
Run the server and confirm all tables are created:
```bash
cd backend && node -e "require('./src/db/dryingSchema'); const db = require('./src/db/schema'); const tables = db.prepare(\"SELECT name FROM sqlite_master WHERE type='table' AND name LIKE 'drying_%'\").all(); console.log('Tables created:', tables.map(t => t.name).sort()); console.log('Count:', tables.length);"
```
Expected: 10 tables listed (drying_logs, drying_chambers, drying_rooms, drying_ref_points, drying_baselines, drying_visits, drying_atmospheric_readings, drying_moisture_readings, drying_equipment, drying_visit_notes).
  </verify>
  <done>All 10 drying_* tables exist with correct columns, foreign keys, cascade deletes, unique constraints, and indexes.</done>
</task>

<task type="auto">
  <name>Task 2: Wire dryingSchema into startup require chain and verify cascades</name>
  <files>backend/src/db/apexSchema.js</files>
  <action>
Add `require('./dryingSchema');` at the bottom of `backend/src/db/apexSchema.js`, AFTER the `console.log('Apex jobs schema initialized');` line and BEFORE the final `module.exports = db;`. This ensures load order: schema.js -> apexSchema.js -> dryingSchema.js (drying tables reference apex_jobs which must exist first).

The line to add:
```javascript
require('./dryingSchema'); // Initialize drying log tables
```

After adding the require, verify the full cascade delete chain works by writing a verification script (run it, do NOT commit it):
1. Insert a test apex_job
2. Insert a drying_log referencing it
3. Insert a drying_chamber referencing the log
4. Insert a drying_room referencing the chamber
5. Insert a drying_ref_point referencing the room and log
6. Delete the apex_job
7. Verify ALL child rows are gone (drying_log, chamber, room, ref_point)

Also verify that `PRAGMA foreign_keys` is ON by running `db.pragma('foreign_keys')` and checking it returns `[{ foreign_keys: 1 }]`.

If foreign keys are OFF, add `db.pragma('foreign_keys = ON');` as the first line after the `const db = require('./schema');` line in dryingSchema.js. (Research note: better-sqlite3 enables foreign keys by default, but verify.)
  </action>
  <verify>
Start the full server and confirm no errors:
```bash
cd backend && node -e "require('./src/db/apexSchema'); console.log('Schema loaded successfully');"
```
Then run a cascade delete test:
```bash
cd backend && node -e "
const db = require('./src/db/apexSchema');
const fk = db.pragma('foreign_keys');
console.log('Foreign keys:', fk);
// Quick cascade test
const { v4: uuidv4 } = require('uuid');
const jobId = uuidv4();
const logId = uuidv4();
const chamberId = uuidv4();
db.prepare('INSERT INTO apex_jobs (id, user_id, name, client_name) VALUES (?, ?, ?, ?)').run(jobId, 'test', 'Test Job', 'Test');
db.prepare('INSERT INTO drying_logs (id, job_id) VALUES (?, ?)').run(logId, jobId);
db.prepare('INSERT INTO drying_chambers (id, log_id, name) VALUES (?, ?, ?)').run(chamberId, logId, 'Chamber 1');
db.prepare('DELETE FROM apex_jobs WHERE id = ?').run(jobId);
const log = db.prepare('SELECT * FROM drying_logs WHERE id = ?').get(logId);
const chamber = db.prepare('SELECT * FROM drying_chambers WHERE id = ?').get(chamberId);
console.log('Log after delete:', log);
console.log('Chamber after delete:', chamber);
console.log('CASCADE:', !log && !chamber ? 'PASS' : 'FAIL');
"
```
Expected: Foreign keys enabled, cascade PASS (both log and chamber are undefined after deleting the job).
  </verify>
  <done>dryingSchema.js is loaded at server startup via apexSchema.js require chain. Foreign key cascades are verified working (deleting an apex_job removes all drying_* children).</done>
</task>

</tasks>

<verification>
1. Server starts without errors: `cd backend && node -e "require('./src/db/apexSchema'); console.log('OK');"`
2. All 10 drying tables exist: query sqlite_master for tables matching 'drying_%'
3. Cascade delete works: insert job -> log -> chamber -> room -> ref_point, delete job, all children gone
4. UNIQUE constraints enforced: inserting duplicate (log_id, ref_number) in drying_ref_points throws error
5. Re-running schema creation is idempotent (no errors on second run)
</verification>

<success_criteria>
- 10 drying_* tables exist with all specified columns
- Foreign key cascades delete children at every level
- UNIQUE(log_id, ref_number) constraint enforced on drying_ref_points
- UNIQUE(log_id, material_code) constraint enforced on drying_baselines
- UNIQUE(visit_id, ref_point_id) constraint enforced on drying_moisture_readings
- Schema creation is idempotent (safe to require multiple times)
- No errors on server startup with existing database
</success_criteria>

<output>
After completion, create `.planning/phases/01-schema-gpp-engine/01-01-SUMMARY.md`
</output>
